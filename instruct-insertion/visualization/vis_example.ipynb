{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import plyfile\n",
    "import pyvista as pv\n",
    "import accelerate\n",
    "from easydict import EasyDict as edict\n",
    "from termcolor import colored\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), '..'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_vscode() -> bool:\n",
    "    for var in os.environ:\n",
    "        if var == \"VSCODE_CWD\":\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "if is_vscode():\n",
    "    print(colored(\"Vscode jupyter DOESN'T support pyvista interative mode\", \"yellow\", force_color=True))\n",
    "    jupyter_backend = \"static\"\n",
    "else:\n",
    "    jupyter_backend = \"trame\"\n",
    "\n",
    "# set this if on remote jupyter\n",
    "# for headless linux users\n",
    "os.environ[\"DISPLAY\"] = \":99.0\"\n",
    "os.environ[\"PYVISTA_OFF_SCREEN\"] = \"true\"\n",
    "# NOTE: vscode remote jupyter does not work with pyvista\n",
    "if not is_vscode():\n",
    "    pv.global_theme.trame.server_proxy_enabled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accelerator = accelerate.Accelerator()\n",
    "device = accelerator.device\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data & models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load existing args\n",
    "PROJECT_TOP_DIR = \"../../tmp_link_saves\"\n",
    "PROJECT_DIR = osp.join(PROJECT_TOP_DIR, \"rpp_fps\")\n",
    "with open(osp.join(PROJECT_DIR, \"config.json.txt\"), \"r\") as f:\n",
    "    args = edict(json.load(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.referit3d.in_out.neural_net_oriented import (\n",
    "    compute_auxiliary_data,\n",
    "    load_referential_data,\n",
    "    load_scan_related_data,\n",
    "    trim_scans_per_referit3d_data_,\n",
    ")\n",
    "# load data\n",
    "SCANNET_PKL_FILE = \"../../datasets/scannet/instruct/global_small.pkl\"\n",
    "REFERIT_CSV_FILE = \"../../datasets/nr3d/nr3d_generative_20230825_final.csv\"\n",
    "all_scans_in_dict, scans_split, class_to_idx = load_scan_related_data(SCANNET_PKL_FILE)\n",
    "referit_data = load_referential_data(args, args.referit3D_file, scans_split)\n",
    "# Prepare data & compute auxiliary meta-information.\n",
    "all_scans_in_dict = trim_scans_per_referit3d_data_(referit_data, all_scans_in_dict)\n",
    "mean_rgb = compute_auxiliary_data(referit_data, all_scans_in_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "# prepare tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(args.bert_pretrain_path)\n",
    "# Prepare the Listener\n",
    "n_classes = len(class_to_idx) - 1  # -1 to ignore the <pad> class\n",
    "pad_idx = class_to_idx[\"pad\"]\n",
    "# Object-type classification\n",
    "class_name_list = list(class_to_idx.keys())\n",
    "\n",
    "class_name_tokens = tokenizer(class_name_list, return_tensors=\"pt\", padding=True)\n",
    "class_name_tokens = class_name_tokens.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.referit3d.datasets import make_data_loaders\n",
    "data_loaders = make_data_loaders(\n",
    "    args=args,\n",
    "    accelerator=accelerator,\n",
    "    referit_data=referit_data,\n",
    "    class_to_idx=class_to_idx,\n",
    "    scans=all_scans_in_dict,\n",
    "    mean_rgb=mean_rgb,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.train_utils import move_batch_to_device_\n",
    "# get random data\n",
    "test_dataset = data_loaders[\"test\"].dataset\n",
    "rand_idx = np.random.randint(0, len(test_dataset))\n",
    "rand_data = test_dataset[rand_idx]\n",
    "collate_fn = data_loaders[\"test\"].collate_fn\n",
    "# get batch\n",
    "batch = collate_fn([rand_data])\n",
    "batch = move_batch_to_device_(batch, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load models with checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPImagePointDiffusionTransformer(\n",
       "  (time_embed): MLP(\n",
       "    (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "    (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    (gelu): GELU(approximate='none')\n",
       "  )\n",
       "  (ln_pre): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (backbone): Transformer(\n",
       "    (resblocks): ModuleList(\n",
       "      (0-11): 12 x ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (c_qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (c_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (attention): QKVMultiheadAttention()\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_post): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (input_proj): Linear(in_features=6, out_features=512, bias=True)\n",
       "  (output_proj): Linear(in_features=512, out_features=12, bias=True)\n",
       "  (clip_embed): Linear(in_features=768, out_features=512, bias=True)\n",
       "  (input_feat_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.referit3d_model.referit3d_net import ReferIt3DNet_transformer\n",
    "from models.point_e_model.diffusion.configs import DIFFUSION_CONFIGS, diffusion_from_config\n",
    "from models.point_e_model.diffusion.sampler import PointCloudSampler\n",
    "from models.point_e_model.models.configs import MODEL_CONFIGS, model_from_config\n",
    "\n",
    "# referit3d model\n",
    "mvt3dvg = ReferIt3DNet_transformer(args, n_classes, class_name_tokens, ignore_index=pad_idx)\n",
    "# point-e model\n",
    "point_e_config = MODEL_CONFIGS[args.point_e_model]\n",
    "point_e_config[\"cache_dir\"] = osp.join(PROJECT_TOP_DIR, \"cache\", \"point_e_model\")\n",
    "point_e_config[\"n_ctx\"] = args.points_per_object\n",
    "point_e = model_from_config(point_e_config, device)\n",
    "point_e_diffusion = diffusion_from_config(DIFFUSION_CONFIGS[args.point_e_model])\n",
    "# move models to gpu\n",
    "mvt3dvg = mvt3dvg.to(device)\n",
    "point_e = point_e.to(device)\n",
    "mvt3dvg.eval()\n",
    "point_e.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model and checkpoints\n",
    "mvt3dvg, point_e = accelerator.prepare(mvt3dvg, point_e)\n",
    "CHECKPOINT_DIR = osp.join(PROJECT_DIR, \"checkpoints\", \"2023-09-12_11-31-26\", \"ckpt_160000\")\n",
    "accelerator.load_state(CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.point_e_model.diffusion.sampler import PointCloudSampler\n",
    "\n",
    "aux_channels = [\"R\", \"G\", \"B\"]\n",
    "sampler = PointCloudSampler(\n",
    "    device=device,\n",
    "    models=[point_e],\n",
    "    diffusions=[point_e_diffusion],\n",
    "    num_points=[args.points_per_object],\n",
    "    aux_channels=aux_channels,\n",
    "    guidance_scale=[3.0],\n",
    "    use_karras=[True],\n",
    "    karras_steps=[64],\n",
    "    sigma_min=[1e-3],\n",
    "    sigma_max=[120],\n",
    "    s_churn=[3],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/link/miniconda3/envs/instruct/lib/python3.9/site-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    ctx_embeds, LOSS, CLASS_LOGITS, LANG_LOGITS, LOCATE_PREDS = mvt3dvg(batch)\n",
    "\n",
    "    prompts = batch[\"text\"]\n",
    "    # stack twice for guided scale\n",
    "    ctx_embeds = torch.cat((ctx_embeds, ctx_embeds), dim=0)\n",
    "    samples_it = sampler.sample_batch_progressive(\n",
    "        batch_size=len(prompts),\n",
    "        ctx_embeds=ctx_embeds,\n",
    "        model_kwargs=dict(texts=prompts),\n",
    "        accelerator=accelerator,\n",
    "    )\n",
    "    # get the last timestep prediction\n",
    "    for last_pcs in samples_it:\n",
    "        pass\n",
    "    last_pcs = last_pcs.permute(0, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace last_pcs with the real point cloud\n",
    "vis_pc = last_pcs.squeeze(0) # (P, 6)\n",
    "\n",
    "pos = vis_pc[:, :3]\n",
    "aux = vis_pc[:, 3:]\n",
    "\n",
    "pred_box_center, pred_box_max_dist = LOCATE_PREDS[0, :3], LOCATE_PREDS[0, 3]\n",
    "\n",
    "coords = pos * pred_box_max_dist + pred_box_center\n",
    "colors = aux.clamp(0, 255).round()  # (P, 3 or 4)\n",
    "vis_pc = torch.cat((coords, colors), dim=-1)  # (P, 6)\n",
    "\n",
    "# Process the color in the scene\n",
    "ctx_pc = batch[\"ctx_pc\"][0] # (# of context, P, 7)\n",
    "ctx_pos = ctx_pc[..., :3] * batch[\"ctx_box_max_dist\"][0][:, None, None] + batch[\"ctx_box_center\"][0][:, None]\n",
    "ctx_color = ctx_pc[..., 3:6].mul(255.0) # color in ctx has been normalized\n",
    "# reshape\n",
    "ctx_pos = ctx_pos.reshape(-1, 3) # (# of context * P, 3)\n",
    "ctx_color = ctx_color.reshape(-1, 3) # (# of context * P, 3)\n",
    "ctx_pc = torch.cat((ctx_pos, ctx_color), dim=-1) # (# of context * P, 6)\n",
    "\n",
    "# Add object into scene\n",
    "vis_pc = torch.cat((vis_pc, ctx_pc), dim=0) # (P + # of context * P, 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07f81480cb9a4b87b855cce25df6db03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Widget(value=\"<iframe src='/proxy/46945/index.html?ui=P_0x7efb0b758070_28&reconnect=auto' style='width: 99%; h…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:aiohttp.websocket:Client protocols [''] don’t overlap server-known ones ()\n"
     ]
    }
   ],
   "source": [
    "# NOTE: TEST entire point clouds\n",
    "plotter = pv.Plotter()\n",
    "# plotter.add_points(batch[\"ctx_pc\"][0, 40, :, :3].cpu().numpy())\n",
    "plotter.add_points(ctx_pc[:, :3].cpu().numpy())\n",
    "plotter.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a pyvista point cloud object and generate mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pyvista point cloud object\n",
    "ctx_cloud = pv.PolyData(ctx_pc[:, :3].cpu().numpy())\n",
    "vis_cloud = pv.PolyData(vis_pc[:, :3].cpu().numpy())\n",
    "\n",
    "# Generate the mesh\n",
    "ctx_mesh = ctx_cloud.delaunay_2d(offset=0.1)\n",
    "vis_mesh = vis_cloud.delaunay_2d()\n",
    "ctx_colors = ctx_pc[:, 3:6].cpu().numpy().astype(np.uint8)\n",
    "vis_colors = vis_pc[:, 3:6].cpu().numpy().astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Plot the ctx mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d29df20d0294c62ba831773958276e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Widget(value=\"<iframe src='/proxy/46945/index.html?ui=P_0x7efb8f55ffd0_26&reconnect=auto' style='width: 99%; h…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:aiohttp.websocket:Client protocols [''] don’t overlap server-known ones ()\n"
     ]
    }
   ],
   "source": [
    "# Plot the mesh\n",
    "plotter = pv.Plotter()\n",
    "plotter.add_mesh(ctx_mesh, scalars=ctx_colors, rgb=True, preference='point')\n",
    "plotter.show(jupyter_backend=jupyter_backend)\n",
    "# Save the plot graph in pdf format\n",
    "# plotter.save_graphic(\"test1.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Plot the vis mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the mesh\n",
    "plotter = pv.Plotter()\n",
    "plotter.add_mesh(vis_mesh, scalars=vis_colors, rgb=True, preference='point')\n",
    "plotter.show(jupyter_backend=jupyter_backend)\n",
    "# Save the plot graph in pdf format\n",
    "# plotter.save_graphic(\"test1.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write and Load point cloud functions (option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plyfile import PlyData, PlyElement\n",
    "\n",
    "def write_ply(points, save_path):\n",
    "    \"\"\"\n",
    "    points: numpy array in shape (N, 6) or (N, 7)\n",
    "    save_name: str end with \".ply\"\n",
    "    \"\"\"\n",
    "    assert points.shape[1] == 6 or points.shape[1] == 7, \"points.shape[1] should be 6 or 7\"\n",
    "    save_path = str(save_path)\n",
    "    assert save_path.endswith(\".ply\"), \"save_name should end with '.ply'\"\n",
    "    points = [\n",
    "        (points[i, 0], points[i, 1], points[i, 2], points[i, 3], points[i, 4], points[i, 5])\n",
    "        for i in range(points.shape[0])\n",
    "    ]\n",
    "    vertex = np.array(\n",
    "        points,\n",
    "        dtype=[\n",
    "            (\"x\", \"f4\"),\n",
    "            (\"y\", \"f4\"),\n",
    "            (\"z\", \"f4\"),\n",
    "            (\"red\", \"f4\"),\n",
    "            (\"green\", \"f4\"),\n",
    "            (\"blue\", \"f4\"),\n",
    "        ],\n",
    "    )\n",
    "    data = PlyElement.describe(vertex, \"vertex\", comments=[\"vertices\"])\n",
    "    PlyData([data]).write(save_path)\n",
    "\n",
    "def read_ply(save_path):\n",
    "    filename = save_path\n",
    "    with open(filename, 'rb') as f:\n",
    "        plydata = plyfile.PlyData.read(f)\n",
    "        num_verts = plydata['vertex'].count\n",
    "        vertices = np.zeros(shape=[num_verts, 6], dtype=np.float32)\n",
    "        vertices[:,0] = plydata['vertex'].data['x']\n",
    "        vertices[:,1] = plydata['vertex'].data['y']\n",
    "        vertices[:,2] = plydata['vertex'].data['z']\n",
    "        vertices[:,3] = plydata['vertex'].data['red']\n",
    "        vertices[:,4] = plydata['vertex'].data['green']\n",
    "        vertices[:,5] = plydata['vertex'].data['blue']\n",
    "    return vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - mesh does not have color information\n",
    "mesh.save('mesh.vtk')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import pyvista as pv\n",
    "import accelerate\n",
    "from easydict import EasyDict as edict\n",
    "from termcolor import colored\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), '..'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mVscode jupyter DOESN'T support pyvista interative mode\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def is_vscode() -> bool:\n",
    "    for var in os.environ:\n",
    "        if var == \"VSCODE_CWD\":\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "if is_vscode():\n",
    "    print(colored(\"Vscode jupyter DOESN'T support pyvista interative mode\", \"yellow\", force_color=True))\n",
    "    jupyter_backend = \"static\"\n",
    "else:\n",
    "    jupyter_backend = \"trame\"\n",
    "\n",
    "# set this if on remote jupyter\n",
    "# for headless linux users\n",
    "os.environ[\"DISPLAY\"] = \":99.0\"\n",
    "os.environ[\"PYVISTA_OFF_SCREEN\"] = \"true\"\n",
    "# NOTE: vscode remote jupyter does not work with pyvista\n",
    "if not is_vscode():\n",
    "    pv.global_theme.trame.server_proxy_enabled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accelerator = accelerate.Accelerator()\n",
    "device = accelerator.device\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write and Load point cloud functions (option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plyfile import PlyData, PlyElement\n",
    "\n",
    "def write_ply(points, save_path):\n",
    "    \"\"\"\n",
    "    points: numpy array in shape (N, 6) or (N, 7)\n",
    "    save_name: str end with \".ply\"\n",
    "    \"\"\"\n",
    "    assert points.shape[1] == 6 or points.shape[1] == 7, \"points.shape[1] should be 6 or 7\"\n",
    "    save_path = str(save_path)\n",
    "    assert save_path.endswith(\".ply\"), \"save_name should end with '.ply'\"\n",
    "    points = [\n",
    "        (points[i, 0], points[i, 1], points[i, 2], points[i, 3], points[i, 4], points[i, 5])\n",
    "        for i in range(points.shape[0])\n",
    "    ]\n",
    "    vertex = np.array(\n",
    "        points,\n",
    "        dtype=[\n",
    "            (\"x\", \"f4\"),\n",
    "            (\"y\", \"f4\"),\n",
    "            (\"z\", \"f4\"),\n",
    "            (\"red\", \"f4\"),\n",
    "            (\"green\", \"f4\"),\n",
    "            (\"blue\", \"f4\"),\n",
    "        ],\n",
    "    )\n",
    "    data = PlyElement.describe(vertex, \"vertex\", comments=[\"vertices\"])\n",
    "    PlyData([data]).write(save_path)\n",
    "\n",
    "def read_ply(save_path):\n",
    "    filename = save_path\n",
    "    with open(filename, 'rb') as f:\n",
    "        plydata = PlyData.read(f)\n",
    "        num_verts = plydata['vertex'].count\n",
    "        vertices = np.zeros(shape=[num_verts, 6], dtype=np.float32)\n",
    "        vertices[:,0] = plydata['vertex'].data['x']\n",
    "        vertices[:,1] = plydata['vertex'].data['y']\n",
    "        vertices[:,2] = plydata['vertex'].data['z']\n",
    "        vertices[:,3] = plydata['vertex'].data['red']\n",
    "        vertices[:,4] = plydata['vertex'].data['green']\n",
    "        vertices[:,5] = plydata['vertex'].data['blue']\n",
    "    return vertices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data & models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load existing args\n",
    "PROJECT_TOP_DIR = \"../../tmp_link_saves\"\n",
    "PROJECT_DIR = osp.join(PROJECT_TOP_DIR, \"fps_axisnorm_rr4_sr3d\")\n",
    "with open(osp.join(PROJECT_DIR, \"config.json.txt\"), \"r\") as f:\n",
    "    args = edict(json.load(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.referit3d.in_out.neural_net_oriented import (\n",
    "    compute_auxiliary_data,\n",
    "    load_referential_data,\n",
    "    load_scan_related_data,\n",
    "    trim_scans_per_referit3d_data_,\n",
    ")\n",
    "# load data\n",
    "SCANNET_PKL_FILE = \"../../datasets/scannet/instruct/global.pkl\"\n",
    "REFERIT_CSV_FILE = \"../../datasets/nr3d/nr3d_generative_20230825_final.csv\"\n",
    "all_scans_in_dict, scans_split, class_to_idx = load_scan_related_data(SCANNET_PKL_FILE)\n",
    "referit_data = load_referential_data(args, args.referit3D_file, scans_split)\n",
    "# Prepare data & compute auxiliary meta-information.\n",
    "all_scans_in_dict = trim_scans_per_referit3d_data_(referit_data, all_scans_in_dict)\n",
    "mean_rgb = compute_auxiliary_data(referit_data, all_scans_in_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "# prepare tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(args.bert_pretrain_path)\n",
    "# Prepare the Listener\n",
    "n_classes = len(class_to_idx) - 1  # -1 to ignore the <pad> class\n",
    "pad_idx = class_to_idx[\"pad\"]\n",
    "# Object-type classification\n",
    "class_name_list = list(class_to_idx.keys())\n",
    "\n",
    "class_name_tokens = tokenizer(class_name_list, return_tensors=\"pt\", padding=True)\n",
    "class_name_tokens = class_name_tokens.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.referit3d.datasets import make_data_loaders\n",
    "data_loaders = make_data_loaders(\n",
    "    args=args,\n",
    "    accelerator=accelerator,\n",
    "    referit_data=referit_data,\n",
    "    class_to_idx=class_to_idx,\n",
    "    scans=all_scans_in_dict,\n",
    "    mean_rgb=mean_rgb,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.train_utils import move_batch_to_device_\n",
    "# get random data\n",
    "test_dataset = data_loaders[\"test\"].dataset\n",
    "rand_idx = np.random.randint(0, len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Place the shoes on your right closest to the wall under the desk, with the door at your back.\n"
     ]
    }
   ],
   "source": [
    "rand_data = test_dataset[rand_idx]\n",
    "print(f\"Original text: {rand_data['text']}\")\n",
    "rand_data_scan, rand_data_target_objs = test_dataset.get_reference_data(rand_idx)[:2]\n",
    "rand_data_3d_objs = rand_data_scan.three_d_objects.copy()\n",
    "rand_data_3d_objs.remove(rand_data_target_objs)\n",
    "# rand_data[\"text\"] = \"Create a chair on the ground in the corner.\"\n",
    "# rand_data[\"tokens\"] = test_dataset.tokenizer(rand_data[\"text\"], max_length=test_dataset.max_seq_len, truncation=True, padding=False)\n",
    "collate_fn = data_loaders[\"test\"].collate_fn\n",
    "# get batch\n",
    "batch = collate_fn([rand_data])\n",
    "batch = move_batch_to_device_(batch, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load models with checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.referit3d_model.referit3d_net import ReferIt3DNet_transformer\n",
    "from models.point_e_model.diffusion.configs import DIFFUSION_CONFIGS, diffusion_from_config\n",
    "from models.point_e_model.diffusion.sampler import PointCloudSampler\n",
    "from models.point_e_model.models.configs import MODEL_CONFIGS, model_from_config\n",
    "\n",
    "# referit3d model\n",
    "mvt3dvg = ReferIt3DNet_transformer(args, n_classes, class_name_tokens, ignore_index=pad_idx)\n",
    "# point-e model\n",
    "point_e_config = MODEL_CONFIGS[args.point_e_model]\n",
    "point_e_config[\"cache_dir\"] = osp.join(PROJECT_TOP_DIR, \"cache\", \"point_e_model\")\n",
    "point_e_config[\"n_ctx\"] = args.points_per_object\n",
    "point_e = model_from_config(point_e_config, device)\n",
    "point_e_diffusion = diffusion_from_config(DIFFUSION_CONFIGS[args.point_e_model])\n",
    "# move models to gpu\n",
    "mvt3dvg = mvt3dvg.to(device).eval()\n",
    "point_e = point_e.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model and checkpoints\n",
    "if args.mode == \"train\":\n",
    "    mvt3dvg = torch.compile(mvt3dvg)\n",
    "mvt3dvg, point_e = accelerator.prepare(mvt3dvg, point_e)\n",
    "CHECKPOINT_DIR = osp.join(PROJECT_DIR, \"checkpoints\", \"2023-09-21_18-18-07\", \"ckpt_240000\")\n",
    "accelerator.load_state(CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.point_e_model.diffusion.sampler import PointCloudSampler\n",
    "\n",
    "aux_channels = [\"R\", \"G\", \"B\"]\n",
    "sampler = PointCloudSampler(\n",
    "    device=device,\n",
    "    models=[point_e],\n",
    "    diffusions=[point_e_diffusion],\n",
    "    num_points=[args.points_per_object],\n",
    "    aux_channels=aux_channels,\n",
    "    guidance_scale=[3.0],\n",
    "    use_karras=[True],\n",
    "    karras_steps=[64],\n",
    "    sigma_min=[1e-3],\n",
    "    sigma_max=[120],\n",
    "    s_churn=[3],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lyy/miniconda3/envs/instruct-insertion/lib/python3.9/site-packages/torch/_inductor/compile_fx.py:90: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n",
      "/home/lyy/miniconda3/envs/instruct-insertion/lib/python3.9/site-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n",
      "/home/lyy/miniconda3/envs/instruct-insertion/lib/python3.9/site-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    ctx_embeds, LOSS, CLASS_LOGITS, LANG_LOGITS, LOCATE_PREDS, pred_xyz = mvt3dvg(batch)\n",
    "\n",
    "    prompts = batch[\"text\"]\n",
    "    # stack twice for guided scale\n",
    "    ctx_embeds = torch.cat((ctx_embeds, ctx_embeds), dim=0)\n",
    "    samples_it = sampler.sample_batch_progressive(\n",
    "        batch_size=len(prompts),\n",
    "        ctx_embeds=ctx_embeds,\n",
    "        model_kwargs=dict(texts=prompts),\n",
    "        accelerator=accelerator,\n",
    "    )\n",
    "    # get the last timestep prediction\n",
    "    for last_pcs in samples_it:\n",
    "        pass\n",
    "    last_pcs = last_pcs.permute(0, 2, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Axis Norm model visualization ###\n",
    "**Only for axis norm model**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For axis_norm model\n",
    "pred_xy, pred_z, pred_radius = pred_xyz\n",
    "pred_xy_topk_bins = pred_xy.topk(5, dim=-1)[1]  # (B, 5)\n",
    "pred_z_topk_bins = pred_z.topk(5, dim=-1)[1]  # (B, 5)\n",
    "pred_x_topk_bins = pred_xy_topk_bins % args.axis_norm_bins  # (B, 5)\n",
    "pred_y_topk_bins = pred_xy_topk_bins // args.axis_norm_bins  # (B, 5)\n",
    "pred_bins = torch.stack(\n",
    "    (pred_x_topk_bins, pred_y_topk_bins, pred_z_topk_bins), dim=-1\n",
    ")  # (B, 5, 3)\n",
    "pred_bins = (pred_bins.float() + 0.5) / args.axis_norm_bins  # (B, 5, 3)\n",
    "(\n",
    "    min_box_center_axis_norm,  # (B, 3)\n",
    "    max_box_center_axis_norm,  # (B, 3)\n",
    ") = (\n",
    "    batch[\"min_box_center_before_axis_norm\"],\n",
    "    batch[\"max_box_center_before_axis_norm\"],\n",
    ")  # all range from [-1, 1]\n",
    "pred_topk_xyz = (\n",
    "    min_box_center_axis_norm[:, None]\n",
    "    + (max_box_center_axis_norm - min_box_center_axis_norm)[:, None] * pred_bins\n",
    ")  # (B, 5, 3)\n",
    "pred_radius = pred_radius.unsqueeze(-1).permute(0, 2, 1).repeat(1, 5, 1)  # (B, 5, 1)\n",
    "# pred_topk_xyz = torch.cat([pred_topk_xyz, pred_radius], dim=-1)  # (B, 5, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose this or the next block\n",
    "# Choose which object position to visualize\n",
    "# The object_idx should between 0 - 4\n",
    "object_idx = 3\n",
    "\n",
    "vis_pc = last_pcs.squeeze(0) # (P, 6)\n",
    "\n",
    "pos = vis_pc[:, :3]\n",
    "aux = vis_pc[:, 3:]\n",
    "\n",
    "pred_box_center, pred_box_max_dist = pred_topk_xyz[:, object_idx, :], pred_radius[:, object_idx, :]\n",
    "\n",
    "# Process the generated point cloud\n",
    "coords = pos * pred_box_max_dist + pred_box_center\n",
    "colors = aux.clamp(0, 255).round()  # (P, 3 or 4)\n",
    "vis_pc = torch.cat((coords, colors), dim=-1)  # (P, 6)\n",
    "vis_pc = vis_pc.unsqueeze(0) # (1, P, 6)\n",
    "vis_pc = vis_pc.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### None-axis Norm model visualization ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace last_pcs with the real point cloud\n",
    "vis_pc = last_pcs.squeeze(0) # (P, 6)\n",
    "\n",
    "pos = vis_pc[:, :3]\n",
    "aux = vis_pc[:, 3:]\n",
    "\n",
    "pred_box_center, pred_box_max_dist = LOCATE_PREDS[0, :3], LOCATE_PREDS[0, 3]\n",
    "\n",
    "# Process the generated point cloud\n",
    "coords = pos * pred_box_max_dist + pred_box_center\n",
    "colors = aux.clamp(0, 255).round()  # (P, 3 or 4)\n",
    "vis_pc = torch.cat((coords, colors), dim=-1)  # (P, 6)\n",
    "vis_pc = vis_pc.unsqueeze(0) # (1, P, 6)\n",
    "vis_pc = vis_pc.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a pyvista point cloud object and generate mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['scene0353_00'],\n",
       " ['Place the shoes on your right closest to the wall under the desk, with the door at your back.'],\n",
       " 9)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['scan_id'], batch[\"text\"], batch[\"ctx_key_padding_mask\"].sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2a38615c74e42feb6fbeea703e906e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Widget(value=\"<iframe src='/proxy/34751/index.html?ui=P_0x7f2b54a88820_14&reconnect=auto' style='width: 99%; h…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:aiohttp.websocket:Client protocols [''] don’t overlap server-known ones ()\n"
     ]
    }
   ],
   "source": [
    "# Create a pyvista point cloud object\n",
    "plotter = pv.Plotter()\n",
    "# add generated objects\n",
    "obj = vis_pc[0]\n",
    "mesh = pv.PolyData(obj[:, :3]).delaunay_3d(alpha=0.005)\n",
    "color = obj[:, 3:6].astype(np.uint8)\n",
    "bound = mesh.bounds\n",
    "plotter.add_box_widget(callback=None, bounds=bound, factor=1.25, outline_translation=False, rotation_enabled=False, color=\"red\")\n",
    "plotter.add_mesh(mesh, scalars=color, rgb=True, preference='point')\n",
    "# add context\n",
    "for obj in rand_data_3d_objs:\n",
    "    mesh = pv.PolyData(obj.pc).delaunay_3d(alpha=1e-3)\n",
    "    color = (obj.color * 255).astype(np.uint8)\n",
    "    plotter.add_mesh(mesh, scalars=color, rgb=True, preference='point')\n",
    "\n",
    "plotter.show(jupyter_backend=jupyter_backend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Earth mover's distance (EMD) evaluation\n",
    "The following code is for EMD evaluation. This procedure is **super slow** and may require several hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/32666 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 33/32666 [00:02<44:12, 12.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([55, 1024, 3])\n",
      "[8, 379, 168, 46, 166, 53, 297, 442, 280, 446, 446, 584, 379, 579, 223, 166, 167, 8, 102, 379, 102, 53, 332, 34, 379, 56, 579, 102, 579, 150, 108, 347, 160, 96, 579, 600, 442, 185, 393, 61, 59, 442, 189, 53, 183, 579, 167, 279, 597, 53, 446, 562, 62, 78, 446]\n",
      "446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "from EMD_evaluation.emd_module import emd_eval\n",
    "\n",
    "# Extract all object of the same class\n",
    "object_dict = {}\n",
    "for data in tqdm.tqdm(test_dataset):\n",
    "    len_of_true_data = (~data[\"ctx_key_padding_mask\"]).sum()\n",
    "    ctx_pc = torch.from_numpy(data[\"ctx_pc\"][:len_of_true_data])[:, :, :3] # (# of context, P, 3)\n",
    "    tgt_pc = torch.from_numpy(data[\"tgt_pc\"])[None, :, :3] # (1, P, 3)\n",
    "    objs = torch.cat((ctx_pc, tgt_pc), dim=0) # (# of context, P, 3)\n",
    "    \n",
    "    ctx_label = data[\"ctx_class\"][:len_of_true_data].tolist() \n",
    "    tgt_label = data[\"tgt_class\"]\n",
    "    ctx_label.append(tgt_label)\n",
    "    labels = ctx_label\n",
    "    for obj, label in zip(objs, labels):\n",
    "        if label not in object_dict:\n",
    "            object_dict[label] = obj[None, :].to(device)\n",
    "        else:\n",
    "            object_dict[label] = torch.cat((object_dict[label], obj[None, :].to(device)), dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.3023, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Compute the EMD\n",
    "# print(object_dict)\n",
    "# print(object_dict[393].shape)\n",
    "# print(batch[\"tgt_class\"])\n",
    "print(emd_eval(coords, object_dict[batch[\"tgt_class\"].item()]))\n",
    "# print(emd_eval(coords, object_dict[53]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description"
    content="CaIPCG: A novel approach to generate indoor object at proper location through user instruction.">
  <meta property="og:title" content="Context-Aware Indoor Point Cloud Object Generation through User Instructions" />
  <meta property="og:description"
    content="CaIPCG: A novel approach to generate indoor object at proper location through user instruction." />
  <meta property="og:url" content="URL OF THE WEBSITE" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />


  <meta name="twitter:title" content="Context-Aware Indoor Point Cloud Object Generation through User Instructions">
  <meta name="twitter:description"
    content="CaIPCG: A novel approach to generate indoor object at proper location through user instruction.">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="CaIPCG,point cloud generation,instructed,context-aware,user instruction,indoor">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Context-Aware Indoor Point Cloud Object Generation through User Instructions</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-2 publication-title">Context-Aware Indoor Point Cloud Object Generation through User
              Instructions</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://mrtater.github.io/" target="_blank">Yiyang Luo</a><sup>*,1</sup>,</span>
              <span class="author-block">
                <a href="https://leonardodalinky.github.io/researcher" target="_blank">Ke Lin</a><sup>*,2</sup>,</span>
              <span class="author-block">
                <a href="https://github.com/guch8017" target="_blank">Chao Gu</a><sup>3</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Nanyang Technological University,</span>
              <span class="author-block"><sup>2</sup>Tsinghua University,</span>
              <span class="author-block"><sup>3</sup>University of Science and Technology of China</span>
              <br>
              <span class="author-block">ACM MM '24</span>
              <span class="eql-cntrb"><small><br><sup>*</sup>Equal Contribution</small></span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2311.16501.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- Supplementary PDF link -->
                <span class="link-block">
                  <a href="static/pdfs/MM2024_CaIPCG_sup.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Supplementary</span>
                  </a>
                </span>

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/AInnovateLab/Context-aware-Indoor-PCG" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2311.16501" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>

                <!-- Dataset Link -->
                <span class="link-block">
                  <a href="https://github.com/AInnovateLab/Context-aware-Indoor-PCG/tree/main/datasets" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Dataset</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Teaser video-->
  <!-- <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <video poster="" id="tree" autoplay controls muted loop height="100%">
          <source src="static/videos/banner_video.mp4" type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered">
          Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat
          pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus.
        </h2>
      </div>
    </div>
  </section> -->
  <!-- End teaser video -->

  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-4 has-text-centered">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Indoor scene modification has emerged as a prominent area within computer vision, particularly for its
              applications in Augmented Reality (AR) and Virtual Reality (VR).
              Traditional methods often rely on pre-existing object databases and predetermined object positions,
              limiting their flexibility and adaptability to new scenarios.
              In response to this challenge, we present a novel end-to-end multi-modal deep neural network capable of
              generating point cloud objects seamlessly integrated with their surroundings, driven by textual
              instructions.
              Our work proposes a novel approach in scene modification by enabling the creation of new environments with
              previously unseen object layouts, eliminating the need for pre-stored CAD models.
              Leveraging Point-E as our generative model, we introduce innovative techniques such as quantized position
              prediction and Top-K estimation to address the issue of false negatives resulting from ambiguous language
              descriptions.
              Furthermore, we conduct comprehensive evaluations to showcase the diversity of generated objects, the
              efficacy of textual instructions, and the quantitative metrics, affirming the realism and versatility of
              our model in generating indoor objects.
              To provide a holistic assessment, we incorporate visual grounding as an additional metric, ensuring the
              quality and coherence of the scenes produced by our model.
              Through these advancements, our approach not only advances the state-of-the-art in indoor scene
              modification but also lays the foundation for future innovations in immersive computing and digital
              environment creation.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->


  <!-- Image carousel -->
  <!-- <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item">
            <img src="static/images/carousel1.jpg" alt="MY ALT TEXT" />
            <h2 class="subtitle has-text-centered">
              First image description.
            </h2>
          </div>
          <div class="item">
            <img src="static/images/carousel2.jpg" alt="MY ALT TEXT" />
            <h2 class="subtitle has-text-centered">
              Second image description.
            </h2>
          </div>
          <div class="item">
            <img src="static/images/carousel3.jpg" alt="MY ALT TEXT" />
            <h2 class="subtitle has-text-centered">
              Third image description.
            </h2>
          </div>
          <div class="item">
            <img src="static/images/carousel4.jpg" alt="MY ALT TEXT" />
            <h2 class="subtitle has-text-centered">
              Fourth image description.
            </h2>
          </div>
        </div>
      </div>
    </div>
  </section> -->
  <!-- End image carousel -->




  <!-- Youtube video -->
  <!-- <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container">
        <h2 class="title is-3">Video Presentation</h2>
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">

            <div class="publication-video">
              <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media"
                allowfullscreen></iframe>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section> -->
  <!-- End youtube video -->


  <!-- Video carousel -->
  <!-- <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <h2 class="title is-3">Another Carousel</h2>
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item item-video1">
            <video poster="" id="video1" autoplay controls muted loop height="100%">
              <source src="static/videos/carousel1.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-video2">
            <video poster="" id="video2" autoplay controls muted loop height="100%">
              <source src="static/videos/carousel2.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-video3">
            <video poster="" id="video3" autoplay controls muted loop height="100%">\
              <source src="static/videos/carousel3.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </section> -->
  <!-- End video carousel -->

  <section class="section hero is-small">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full">
          <h2 class="title is-4 has-text-centered">Summary</h2>
          <div class="content">
            <div class="has-text-justified">
              <img src=" static/images/PISA-Intro.svg"
                alt="Our model generates a couch that is positioned close to the television in response to the query and makes it consistent with the rest of the scene, i.e., the orientation, size, and overlap with other objects in certain cases."
                class="center-image" width="50%" />
              <br>
              <p>
                In summary, the contributions of our work are as follows:
              <ul>
                <li>We generate a new dataset for scene modification tasks by designing a GPT-aided data pipeline for
                  paraphrasing the descriptive texts in ReferIt3D dataset to generative instructions, referred to
                  <strong>Nr3D-SA</strong> and <strong>Sr3D-SA</strong> datasets.
                </li>
                <li>We propose an <strong>end-to-end</strong> multi-modal diffusion-based deep neural network model for
                  generating
                  in-door 3D objects into specific scenes according to input instructions.</li>
                <li>We propose <strong>quantized position prediction</strong>, a simple but effective technique to
                  predict Top-K
                  candidate positions, which mitigates false negative problems arising from the ambiguity of language
                  and provides reasonable options.</li>
                <li>We introduce the visual grounding task as an evaluation strategy to assess the quality of a
                  generated scene and integrate several metrics to evaluate the generated objects.</li>
              </ul>
              </p>
            </div>
          </div>

        </div>
      </div>
    </div>
    </div>
  </section>


  <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container  is-max-desktop">
        <h2 class="title is-4 has-text-centered">Methodology & Pipeline</h2>
        <div class="has-text-justified">
          <img src="static/images/PISA-General_Pipeline.svg" alt="Overview pipeline of CaIPCG." class="center-image"
            width="90%" />
          <br>
          <p>
          <ol>
            <li><b>Data Pipeline:</b> A large language model (LLM) is used to paraphrase the descriptive text,
              combined with rule-based and manual corrections.</li>
            <li><b>Model Pipeline:</b> Upon receiving generative text as a query and point
              cloud input, our model integrates both object and language features to predict the final position.
              Besides,
              the language features are aligned across the model. The amalgamated features are then processed through
              the
              Point-E model to generate a realistic object.</li>
          </ol>
          </p>
        </div>
      </div>
    </div>
    </div>
  </section>



  <section class="section hero is-small">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full">
          <div class="content">
            <h2 class="title is-4 has-text-centered">Experiments</h2>
            <img src="static/images/PISA-Comparison.svg" alt="Scene before and after modification."
              class="center-image" />
            <br>
            <div class="has-text-centered">
              <strong><em>Consistency with Surroundings</em></strong>
              <p>
                Most generated point clouds are located close to the reference and the shapes are consistent with
                the instructions.
              </p>
            </div>
            <hr />
            <img src="static/images/PISA-Diversity.svg" alt="Diversity." class="center-image" />
            <br>
            <div class="has-text-centered">
              <strong><em>Diversity of Generations</em></strong>
              <p>
                While maintaining consistency with the surrounding environment and instructions, our method creates
                meaningful variances in both shape and color.
              </p>
            </div>
            <hr />
            <img src="static/images/PISA-Query_Effect.svg"
              alt="Generated objects under instructions with slight variations." class="center-image" />
            <br>
            <div class="has-text-centered">
              <strong><em>Effectiveness of Instructions</em></strong>
              <p>
                Generated objects can exhibit variations in color, shape, and location while remaining aligned with the
                provided instructions and the context of surrounding objects.
              </p>
            </div>
            <hr />
            <img src="static/images/CaIPCG_VG.png" alt="Visual grounding analysis." class="center-image" width="50%" />
            <br>
            <div class="has-text-centered">
              <strong><em>Quality through Visual Grounding Analysis</em></strong>
              <p>
                Our model is capable of generating scenes that are not only consistent but also easily recognizable by
                visual grounding models trained on the original dataset.
              </p>
            </div>
            <!-- <div class="container mt-4">
              <div class="alert alert-danger" role="alert">
                <strong>Implications:</strong> SoTA LLMs that use LoRA for alignment fine-tuning are vulnerable to
                Pre-FT weight recovery attacks
              </div>
            </div> -->
          </div>
        </div>
      </div>
  </section>




  <!-- Paper poster -->
  <!-- <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container">
        <h2 class="title">Poster</h2>

        <iframe src="static/pdfs/sample.pdf" width="100%" height="550">
        </iframe>

      </div>
    </div>
  </section> -->
  <!--End paper poster -->


  <!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
@article{luo2024CaIPCG,
  title={Context-Aware Indoor Point Cloud Object Generation through User Instructions},
  author={Luo, Yiyang and Lin, Ke and Gu, Chao},
  journal={arXiv preprint arXiv:2311.16501},
  year={2023}
}
      </code></pre>
    </div>
  </section>
  <!--End BibTex citation -->


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This page was modified by K. Lin using the <a
                href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project
                Page Template</a> which was adopted from the <a href="https://nerfies.github.io"
                target="_blank">Nerfies</a> project page.
              This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"
                target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

</body>

</html>